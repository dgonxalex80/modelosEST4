---
title: <span style="color:#034a94">**Análisis de Correspondencia**</span>
author: "Modelos Estadísticos para la toma de decisiones"
date: " "
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA)
```

</br>

El Análisis de Corespondencia es un método estadístico utilizado para representar posibles asociaciones entre  variables categóticas, es decir la asociación entre sus cagorías, con el fin establecer si existe, patromes o estructuras en los datos 

</br>

Este método estadístico es de tipo exploratorio y complementario de otros tipos de análisis como los modelos de regresión logistico.

</br>

La creación de esta herramienta estadistica se le atribulle al matemático y estadíostico francés  Jean-Paul Benzécri al final de la décade de los noventa. A lo largo del tiempo este método se ha empleado por varios campos del conocimiento con diversos nombres como : Escalonamiento óptimo, Análisis canónico, Método de los promedios recíprocos, Puntuaciones aditivas, Puntuaciones apropiadas, Ponderaciones de Guttman, Teoria de cuantificación de Hayashi, Regresión lineal simultánea, Análisis factorial de correspondencia, Biplot, Escalado dual.

</br>

Iniciaremos la explicación del método asociandolo con el caso de dos variables A y B y un test de independencia para tablas de contingencia chi-cuadrado, la cual consiste en obtener el estadístico de prueba mediente la sumatoria de  la razón de las diferencias al cuadrado de los valores esperados y los valores observados y los valores observados, llamada tambien distancia de Pearson  :

</br></br>

$$\lambda = \sum_{i=1}^{m} \sum_{j=1}^{n} \dfrac{(n_{ij} - n_{i.} n_{.j}/n)^2}{n_{i.}n_{.j}/n}$$
</br>

Donde 

<center>
![](img/tablaab.png){width=50%}
</center>

</br></br>


Supongamos que tenemos la siguiente tabla de contingencia, como resultado de una  encuesta que contenia dos preguntas: su actividad laboral y si habian sufrido o no de alguna enfermedad relacionadas con el estrés. 



<center>
![](img/tabla2.png){width=33%}
</center>

Con esta información podremos obtener una metrica que nos permita agrupar las profesiones conformando grupos semejantes


<center>
![](img/tabla3.png){width=35%}
</center>

</br></br>

<center>

![](img/pocisiones.png){width=100%}
</center>

</br></br>

La representación gráfica de estos valores permite visualizar que podrían existir tres grupos en cuanto a enfermedades relacionadas con el estrés :

* Empleado público, empreado estatal y ama de casa
* Docente
* Empleado bancario

</br></br>

## **Análisis de correspondencia simple**

</br>

Ahora utilicemos la metrica $\lambda$ de Pearson para identificar si existe asociacion entre las variables $A$ y $B$. Para ello se parte de una tabla cruzada o de doble entrada donde se visializan las frecuencias conjuntas para todas las combinaciones posibles entre las categorias de ambas variables.

</br></br>

<center>

![](img/tablaAB.png){width=50%}
</center>

```{r, warning=FALSE, message=FALSE}
library(tidyr)
m <- c(50,9,41,4,315,40,147,11,24,6,14,1,4012,459,1539,124) %>%
      matrix(., ncol=4) 
colnames(m) <- c( "b1", "b2", "b3", "b4")
rownames(m) <- c( "a1", "a2", "a3", "a4")
m
```

Mediante la prueba chi.cuadrado se obtienen dos matrices con las que posteriormente se construiran las coordenadas que serán representadas en un plano cartesiano para permitir ver la cercania o no entre las categorias de ambas variables


```{r, warning=FALSE}
chisq.test(m)
```

Los resultados de la prueba indican que las variables **A** y **B** no son independientes y que por tanto existen relaciones entre sus categorias. 


Iniciaremos visualizando la matriz de valores observados 

```{r, warning=FALSE}
chisq.test(m)$observed
```


Y la matriz de valores esperados. Estos valores corresponde a los valores esperados en caso de que se cumpla la hipotesis nula que afirma que las dos variables son independientes haciendo que se cumpla que :


$$n_{11} = \dfrac{n_{.1} \times n_{1.} }{n} = \dfrac{4401 \times 104}{6796} = 67.34903$$


```{r, warning=FALSE}
chisq.test(m)$expected
```

A partir de los valores observados y los valores esperados se calcula la matriz de discrepancias 

$$ d_{11} = \dfrac{(50-67.349029)^2}{67.349029} = 4.469089$$


```{r, warning=FALSE}
Obs <- chisq.test(m)$observed
Esp <- chisq.test(m)$expected
Discrepancias <- (Obs-Esp)^2/Esp
Discrepancias
```

<br/>

La suma total de los valores de la matriz de discrepacias constituye el valor del estadisco chi-cuadrado ($\lambda = 21.359$)


```{r}
sum(Discrepancias)
```


<br/>

Ahora fijaremos como propósito determinar las **coordenadas** para cada una de las clases de las dos variables (a1,a2,a3,a4,b1,b2,b3 y b4) a partir de la matriz de discrepancias que llamaremos **C**. Para ello se debe realizar la factorización de la matriz **C** como .


$$C = U \hspace{.2cm} D \hspace{.2cm} V^{t}$$


```{r}
C <- Discrepancias
U <- eigen(C%*%t(C))$vectors
U
```


```{r}
V <- eigen(t(C) %*% C)$vectors
V
```


```{r}
a <- sqrt(eigen(C%*%t(C))$values)
a
```


```{r}
D <- diag(a)
D
```



```{r}
-U%*%D%*%t(V)
C
```

```{r}
udvt=svd(Discrepancias)
U <-udvt$u
D <-udvt$d
V <-udvt$v
```

```{r}
U
```

```{r}
D
```

```{r}
V
```

```{r}
coord_filas <- U %*% sqrt(D)
coord_filas
```


```{r}
coord_columnas <- t(V) %*% sqrt(D)
coord_columnas
```

Finalmente se representa los resultados obtenidos en un plano cartesiano donde se pueden visualizar las relaciones entre las categorias de las dos variables
